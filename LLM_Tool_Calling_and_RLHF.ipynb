{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPRJ6d+KhOqNDkPhQ3WF3Un",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arpit1118/Post-Training-LLMs-with-RL/blob/main/LLM_Tool_Calling_and_RLHF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sympy as sp\n",
        "import json\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# --- Global Variables ---\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "tokenizer = None\n",
        "model = None\n",
        "\n",
        "# --- Math Solver Class (Tool Implementation) ---\n",
        "class MathSolver:\n",
        "    def __init__(self, variable='x'):\n",
        "        # Define the variable 'x' for symbolic math\n",
        "        self.x = sp.Symbol(variable)\n",
        "\n",
        "    def solve_equation(self, equation_str):\n",
        "        \"\"\"Solves an algebraic equation for 'x'.\"\"\"\n",
        "        try:\n",
        "            # Separate LHS and RHS, then move all terms to the left\n",
        "            if '=' in equation_str:\n",
        "                lhs, rhs = equation_str.split('=')\n",
        "                expr = sp.sympify(lhs) - sp.sympify(rhs)\n",
        "            else:\n",
        "                expr = sp.sympify(equation_str) # Assume it's already an expression equal to zero\n",
        "\n",
        "            roots = sp.solve(expr, self.x)\n",
        "            numeric = [sp.N(r) for r in roots]\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"symbolic\": [str(r) for r in roots],\n",
        "                \"numeric\": [str(n) for n in numeric]\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"success\": False, \"error\": str(e)}\n",
        "\n",
        "    def evaluate_expression(self, expr_str):\n",
        "        \"\"\"Evaluates a basic math expression numerically.\"\"\"\n",
        "        try:\n",
        "            # Use evalf() for numeric evaluation\n",
        "            result = sp.sympify(expr_str).evalf()\n",
        "            return {\"success\": True, \"result\": str(result)}\n",
        "        except Exception as e:\n",
        "            return {\"success\": False, \"error\": str(e)}\n",
        "\n",
        "math_solver_instance = MathSolver()"
      ],
      "metadata": {
        "id": "qbetoqUoI_--"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Function to load the model ---\n",
        "def load_qwen_model():\n",
        "    \"\"\"Loads the Qwen model and tokenizer.\"\"\"\n",
        "    global tokenizer, model\n",
        "    try:\n",
        "        print(f\"Loading Qwen model: {model_name}...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        # Use float32 for CPU compatibility\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\n",
        "        model.to('cpu').eval()\n",
        "        print(\"Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to load Qwen model/tokenizer. Error: {e}\")\n",
        "\n",
        "# Map the function names to their executable counterparts\n",
        "AVAILABLE_TOOLS = {\n",
        "    \"solve_equation\": math_solver_instance.solve_equation,\n",
        "    \"evaluate_expression\": math_solver_instance.evaluate_expression,\n",
        "}\n",
        "\n",
        "# Define the tool specifications in Qwen's expected JSON format\n",
        "MATH_TOOL_DEFINITION = \"\"\"\n",
        "[\n",
        "    {\n",
        "        \"name\": \"solve_equation\",\n",
        "        \"description\": \"Solves an algebraic equation for the variable 'x'. Use this for problems containing an equals sign, e.g., 'x**2 - 4 = 0'.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"equation_str\": {\"type\": \"string\", \"description\": \"The equation to solve, e.g., 'x**2 - 4 = 0'.\"}\n",
        "            },\n",
        "            \"required\": [\"equation_str\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"evaluate_expression\",\n",
        "        \"description\": \"Calculates the numeric result of a math expression. Use this for calculations without an equals sign, e.g., '5*6' or 'sqrt(9)'.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"expr_str\": {\"type\": \"string\", \"description\": \"The expression to evaluate, e.g., '2 + 3 * 4' or 'sqrt(9)'.\"}\n",
        "            },\n",
        "            \"required\": [\"expr_str\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NeFhr4V2J0Ss"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tool Execution Function ---\n",
        "def execute_tool_call(tool_name, tool_args):\n",
        "    \"\"\"Executes the specified tool with arguments.\"\"\"\n",
        "    tool_func = AVAILABLE_TOOLS.get(tool_name)\n",
        "    if tool_func:\n",
        "        try:\n",
        "            # Pass arguments directly to the MathSolver functions\n",
        "            return tool_func(**tool_args)\n",
        "        except Exception as e:\n",
        "            return {\"success\": False, \"error\": str(e)}\n",
        "    else:\n",
        "        return {\"success\": False, \"error\": f\"Tool '{tool_name}' not found.\"}\n",
        "\n",
        "# --- Tool Call Extraction Function (Robust Parsing) ---\n",
        "def extract_tool_call_json(response_text):\n",
        "    \"\"\"\n",
        "    Attempts to extract the tool call JSON, searching first for Qwen tags,\n",
        "    then falling back to looking for bare JSON content.\n",
        "    Returns (tool_call_text, tool_call_match_object).\n",
        "    \"\"\"\n",
        "    # 1. Search for the standard Qwen action tags\n",
        "    primary_match = re.search(r\"(<\\|action_start\\|>)(.*?)(\\<\\|action_end\\|>)\", response_text, re.DOTALL)\n",
        "    if primary_match:\n",
        "        return primary_match.group(0), primary_match\n",
        "\n",
        "    # 2. Fallback: Search for standalone JSON with \"name\" and \"arguments\"\n",
        "    json_search = re.search(r\"(\\{[\\s\\n]*\\\"name\\\".*?\\\"arguments\\\".*?\\}(?:\\n|\\s|\\}))\", response_text, re.DOTALL)\n",
        "    if json_search:\n",
        "        raw_json_content = json_search.group(1).strip().replace(\"<|im_end|>\", \"\").strip()\n",
        "        try:\n",
        "            # Validate JSON and construct a proper tool call string for the main logic\n",
        "            json.loads(raw_json_content)\n",
        "            tool_call_text = f\"<|action_start|>\\n{raw_json_content}\\n<|action_end|>\"\n",
        "\n",
        "            # Create a simplified mock match object\n",
        "            class MockMatch:\n",
        "                def group(self, index):\n",
        "                    if index == 0: return tool_call_text\n",
        "                    if index == 2: return raw_json_content\n",
        "                    raise IndexError\n",
        "\n",
        "            print(\"[Warning: Fallback JSON parsing successful. Model output was missing action tags.]\")\n",
        "            return tool_call_text, MockMatch()\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "5nPFboSNJ2fJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = f\"\"\"\n",
        "You are a helpful and precise assistant. You have access to the following math-solving tools:\n",
        "{MATH_TOOL_DEFINITION}\n",
        "When the user asks a mathematical question (equation solving or calculation), you **must** call the appropriate tool.\n",
        "You **must** respond with the tool call exactly in the following format:\n",
        "<|action_start|>\n",
        "{{\n",
        "  \"name\": \"tool_name\",\n",
        "  \"arguments\": {{\n",
        "    \"arg1\": \"value1\",\n",
        "    \"arg2\": \"value2\"\n",
        "  }}\n",
        "}}\n",
        "<|action_end|>\n",
        "Do not output any introductory or conversational text before the tool call. Only after receiving the tool's result should you provide a natural language answer.\n",
        "If the user's request is not a math problem, answer directly without a tool call.\n",
        "\"\"\"\n",
        "\n",
        "def generate_response(prompt):\n",
        "    \"\"\"Generates the Qwen model's response, handling tool calls in a ReAct loop.\"\"\"\n",
        "\n",
        "    if not model or not tokenizer:\n",
        "        return \"ERROR: Model not loaded.\"\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # --- Step 1: Initial Generation (Tool Call) ---\n",
        "    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
        "    output = model.generate(input_ids, max_new_tokens=512, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "    response_text = tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=False).strip()\n",
        "\n",
        "    tool_call_text, tool_call_match = extract_tool_call_json(response_text)\n",
        "\n",
        "    if tool_call_match:\n",
        "        print(\"\\n[--- Tool Call Detected ---]\")\n",
        "        try:\n",
        "            tool_call_json_str = tool_call_match.group(2).strip()\n",
        "            tool_call_json = json.loads(tool_call_json_str)\n",
        "            tool_name = tool_call_json[\"name\"]\n",
        "            tool_args = tool_call_json.get(\"arguments\", {})\n",
        "\n",
        "            print(f\"  Tool: {tool_name}, Args: {tool_args}\")\n",
        "            tool_output = execute_tool_call(tool_name, tool_args)\n",
        "            print(f\"  Tool Result: {tool_output}\")\n",
        "\n",
        "            # --- Step 2: Rerun with Tool Output (Observation) ---\n",
        "            # 1. Add model's tool-call message (Action)\n",
        "            messages.append({\"role\": \"assistant\", \"content\": tool_call_text})\n",
        "            # 2. Add tool's result message (Observation)\n",
        "            messages.append({\"role\": \"assistant\", \"content\": f\"The result of calling {tool_name} is: {tool_output}\"})\n",
        "\n",
        "            print(\"[--- Rerunning model for final answer ---]\")\n",
        "            final_input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
        "            final_output = model.generate(final_input_ids, max_new_tokens=512, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "            final_response_text = tokenizer.decode(final_output[0][final_input_ids.shape[1]:], skip_special_tokens=False).strip()\n",
        "\n",
        "            # Clean special tokens from the final response\n",
        "            final_response_text = re.sub(r\"<\\|action_start\\|>.*?<\\|action_end\\|>\", \"\", final_response_text, flags=re.DOTALL).strip()\n",
        "            return final_response_text.replace(\"<|im_end|>\", \"\").replace(\"<|im_start|>\", \"\").strip()\n",
        "\n",
        "        except (json.JSONDecodeError, KeyError) as e:\n",
        "            print(f\"[Warning: Tool execution/parsing failed. Returning raw output. Error: {e}]\")\n",
        "\n",
        "    # Fallback: Clean and return the initial raw response\n",
        "    return response_text.replace(\"<|im_end|>\", \"\").replace(\"<|im_start|>\", \"\").strip()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    load_qwen_model()\n",
        "    print(\"\\nQwen Assistant with Math Solver Tool Ready. Type 'exit' to quit.\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"\\nUser >>> \")\n",
        "            if user_input.lower() in ['exit', 'quit']:\n",
        "                break\n",
        "\n",
        "            response = generate_response(user_input)\n",
        "            print(f\"Qwen <<< {response}\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nExiting...\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn unexpected error occurred: {e}\")\n",
        "            break"
      ],
      "metadata": {
        "id": "alMhWsghKG_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e0b18c-53a8-4e4b-debf-dd7e27d7d77e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Qwen model: Qwen/Qwen2.5-1.5B-Instruct...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n",
            "\n",
            "Qwen Assistant with Math Solver Tool Ready. Type 'exit' to quit.\n",
            "\n",
            "User >>> What is the capital of France?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qwen <<< France's capital is Paris.\n",
            "\n",
            "User >>> What is the capital of India and Germany?\n",
            "Qwen <<< India's capital is New Delhi, and Germany's capital is Berlin.\n",
            "\n",
            "User >>> solve this expression: 5*6*7\n",
            "\n",
            "[--- Tool Call Detected ---]\n",
            "  Tool: evaluate_expression, Args: {'expr_str': '5*6*7'}\n",
            "  Tool Result: {'success': True, 'result': '210.000000000000'}\n",
            "[--- Rerunning model for final answer ---]\n",
            "Qwen <<< The result of evaluating the expression 5*6*7 is 210.\n",
            "\n",
            "User >>> solve this: sqrt(x + 5) = x - 1\n",
            "\n",
            "[--- Tool Call Detected ---]\n",
            "  Tool: solve_equation, Args: {'equation_str': 'sqrt(x + 5) = x - 1'}\n",
            "  Tool Result: {'success': True, 'symbolic': ['4'], 'numeric': ['4.00000000000000']}\n",
            "[--- Rerunning model for final answer ---]\n",
            "Qwen <<< The solution to the equation sqrt(x + 5) = x - 1 is x = 4.\n",
            "\n",
            "User >>> solve this: tan(x) - sqrt(3) = 0\n",
            "\n",
            "[--- Tool Call Detected ---]\n",
            "  Tool: solve_equation, Args: {'equation_str': 'tan(x) - sqrt(3) = 0'}\n",
            "  Tool Result: {'success': True, 'symbolic': ['pi/3'], 'numeric': ['1.04719755119660']}\n",
            "[--- Rerunning model for final answer ---]\n",
            "Qwen <<< The solution to the equation tan(x) - sqrt(3) = 0 is x = pi/3 radians or approximately 1.047 radians.\n",
            "\n",
            "User >>> quit\n"
          ]
        }
      ]
    }
  ]
}