{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arpit1118/Post-Training-LLMs-with-RL/blob/main/LLM_using_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jRp5V8SG4XD",
        "outputId": "992d3112-acde-451a-e052-f5fd2aec4833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Core libraries\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Transformers from Hugging Face\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Evaluation and logging\n",
        "import time\n",
        "import traceback\n",
        "import logging\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Using Qwen2.5 model\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer =  AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hn7kDzJcHR5w",
        "outputId": "8d571ecb-6789-4e1d-8357-9d8aeafec4fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Math Solver Ready. Type 'exit' to quit.\n",
            ">>> QUIT\n"
          ]
        }
      ],
      "source": [
        "import sympy as sp\n",
        "\n",
        "class MathSolver:\n",
        "    def __init__(self, variable='x'):\n",
        "        self.x = sp.Symbol(variable)\n",
        "\n",
        "    def solve_equation(self, equation_str):\n",
        "        \"\"\"\n",
        "        Solves an equation like 'x**2 - 4 = 0' or 'sin(x) = 0'\n",
        "        Returns symbolic and numeric solutions\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if '=' in equation_str:\n",
        "                lhs, rhs = equation_str.split('=')\n",
        "                expr = sp.sympify(lhs) - sp.sympify(rhs)\n",
        "            else:\n",
        "                expr = sp.sympify(equation_str)\n",
        "\n",
        "            roots = sp.solve(expr, self.x)\n",
        "            numeric = [sp.N(r) for r in roots]\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"symbolic\": roots,\n",
        "                \"numeric\": numeric,\n",
        "                \"error\": None\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"symbolic\": None,\n",
        "                \"numeric\": None,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "    def evaluate_expression(self, expr_str):\n",
        "        \"\"\"\n",
        "        Evaluates a basic math expression like '2 + 3 * 4'\n",
        "        \"\"\"\n",
        "        try:\n",
        "            result = sp.sympify(expr_str).evalf()\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"result\": result,\n",
        "                \"error\": None\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"result\": None,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "# Example REPL\n",
        "if __name__ == \"__main__\":\n",
        "    solver = MathSolver()\n",
        "    print(\"Math Solver Ready. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        inp = input(\">>> \")\n",
        "        if inp.lower() in ['exit', 'quit']:\n",
        "            break\n",
        "        if '=' in inp:\n",
        "            out = solver.solve_equation(inp)\n",
        "            if out[\"success\"]:\n",
        "                print(f\"Symbolic: {out['symbolic']}\")\n",
        "                print(f\"Numeric: {out['numeric']}\")\n",
        "            else:\n",
        "                print(\"Error:\", out[\"error\"])\n",
        "        else:\n",
        "            out = solver.evaluate_expression(inp)\n",
        "            if out[\"success\"]:\n",
        "                print(\"Result:\", out[\"result\"])\n",
        "            else:\n",
        "                print(\"Error:\", out[\"error\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from functools import lru_cache\n",
        "\n",
        "class LLMMathWrapper:\n",
        "    def __init__(self, model, tokenizer, math_solver):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.math_solver = math_solver\n",
        "\n",
        "    def is_math_prompt(self, prompt):\n",
        "        prompt = prompt.strip()\n",
        "\n",
        "        # Removed punctuation for better word analysis\n",
        "        cleaned = re.sub(r'[^\\w\\s]', '', prompt.lower())\n",
        "\n",
        "        # Quick pass for math symbols or math keywords\n",
        "        if re.search(r'[\\d\\+\\-\\*/\\^=()]', prompt):\n",
        "            math_keywords = {\"solve\", \"evaluate\", \"simplify\", \"integrate\", \"differentiate\", \"factor\"}\n",
        "            if any(kw in cleaned for kw in math_keywords):\n",
        "                return True\n",
        "\n",
        "            words = cleaned.split()\n",
        "            if len(words) <= 5 or sum(w.isdigit() or w in {'x', 'y'} for w in words) >= len(words) // 2:\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    @lru_cache(maxsize=128)  # Optional: speeds up repeated LLM queries\n",
        "    def generate_with_llm(self, prompt):\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        outputs = self.model.generate(inputs, max_length=100, num_return_sequences=1)\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    def run(self, prompt):\n",
        "        if self.is_math_prompt(prompt):\n",
        "            if '=' in prompt:\n",
        "                result = self.math_solver.solve_equation(prompt)\n",
        "                if result[\"success\"]:\n",
        "                    return f\"Symbolic: {result['symbolic']}\\nNumeric: {result['numeric']}\"\n",
        "                else:\n",
        "                    return f\"Math Error: {result['error']}\"\n",
        "            else:\n",
        "                result = self.math_solver.evaluate_expression(prompt)\n",
        "                if result[\"success\"]:\n",
        "                    return f\"Result: {result['result']}\"\n",
        "                else:\n",
        "                    return f\"Math Error: {result['error']}\"\n",
        "        else:\n",
        "            return self.generate_with_llm(prompt)\n"
      ],
      "metadata": {
        "id": "Cm7W7H1tO6gs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapper = LLMMathWrapper(model, tokenizer, solver)\n",
        "print(wrapper.run(\"What is the capital of India\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6r6l9G_s7iI",
        "outputId": "ef74cd36-a66f-48f9-b8bc-e5ee1dcc4a1e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the capital of India?\n",
            "What is the capital of India?\n",
            "Do those questions have the same meaning?\n",
            "Pick from:\n",
            " (A). no\n",
            " (B). yes\n",
            "(B).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wrapper.run(\"What is the capital of France\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVzppPJduLmJ",
        "outputId": "2e9cc40b-8f45-464c-f474-9548f771059f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the capital of France?\n",
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wrapper.run(\"sqrt(x) + sqrt(x + 1) = 2\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hElaCHEsvL5-",
        "outputId": "e2ecd0d0-c533-4286-a2dd-2b50aeb29697"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbolic: [9/16]\n",
            "Numeric: [0.562500000000000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wrapper.run(\"(x - sqrt(3))*(x + sqrt(3)) = 0\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id_F1QKlvSUk",
        "outputId": "9900c5c0-26e8-476c-c224-8bdf9795ae66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbolic: [-sqrt(3), sqrt(3)]\n",
            "Numeric: [-1.73205080756888, 1.73205080756888]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wwdZV0t80eZi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoLioEvGVTaGHBFgcipDpm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}