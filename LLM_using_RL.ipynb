{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arpit1118/Post-Training-LLMs-with-RL/blob/main/LLM_using_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jRp5V8SG4XD",
        "outputId": "75512e7a-2abd-404c-cd33-4e0099de0378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 896)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Core libraries\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Transformers from Hugging Face\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Evaluation and logging\n",
        "import time\n",
        "import traceback\n",
        "import logging\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Using Qwen2.5 model\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer =  AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "device = torch.device(\"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Hn7kDzJcHR5w",
        "outputId": "0051d11e-2cd4-4594-9492-452c03928fd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Math Solver Ready. Type 'exit' to quit.\n",
            ">>> x**3 - 6*x**2 + 11*x - 6 = 0\n",
            "Symbolic: [1, 2, 3]\n",
            "Numeric: [1.00000000000000, 2.00000000000000, 3.00000000000000]\n",
            ">>> sqrt(x + 5) = x - 1\n",
            "Symbolic: [4]\n",
            "Numeric: [4.00000000000000]\n",
            ">>> tan(x) - sqrt(3) = 0\n",
            "Symbolic: [pi/3]\n",
            "Numeric: [1.04719755119660]\n",
            ">>> sin(x)**2 + cos(x)**2 - 1 = 0\n",
            "Symbolic: []\n",
            "Numeric: []\n",
            ">>> sqrt(2)**2 + log(exp(1))\n",
            "Result: 3.00000000000000\n",
            ">>> quit\n"
          ]
        }
      ],
      "source": [
        "import sympy as sp\n",
        "\n",
        "class MathSolver:\n",
        "    def __init__(self, variable='x'):\n",
        "        self.x = sp.Symbol(variable)\n",
        "\n",
        "    def solve_equation(self, equation_str):\n",
        "        \"\"\"\n",
        "        Solves an equation like 'x**2 - 4 = 0' or 'sin(x) = 0'\n",
        "        Returns symbolic and numeric solutions\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if '=' in equation_str:\n",
        "                lhs, rhs = equation_str.split('=')\n",
        "                expr = sp.sympify(lhs) - sp.sympify(rhs)\n",
        "            else:\n",
        "                expr = sp.sympify(equation_str)\n",
        "\n",
        "            roots = sp.solve(expr, self.x)\n",
        "            numeric = [sp.N(r) for r in roots]\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"symbolic\": roots,\n",
        "                \"numeric\": numeric,\n",
        "                \"error\": None\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"symbolic\": None,\n",
        "                \"numeric\": None,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "    def evaluate_expression(self, expr_str):\n",
        "        \"\"\"\n",
        "        Evaluates a basic math expression like '2 + 3 * 4'\n",
        "        \"\"\"\n",
        "        try:\n",
        "            result = sp.sympify(expr_str).evalf()\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"result\": result,\n",
        "                \"error\": None\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"result\": None,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    solver = MathSolver()\n",
        "    print(\"Math Solver Ready. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        inp = input(\">>> \")\n",
        "        if inp.lower() in ['exit', 'quit']:\n",
        "            break\n",
        "        if '=' in inp:\n",
        "            out = solver.solve_equation(inp)\n",
        "            if out[\"success\"]:\n",
        "                print(f\"Symbolic: {out['symbolic']}\")\n",
        "                print(f\"Numeric: {out['numeric']}\")\n",
        "            else:\n",
        "                print(\"Error:\", out[\"error\"])\n",
        "        else:\n",
        "            out = solver.evaluate_expression(inp)\n",
        "            if out[\"success\"]:\n",
        "                print(\"Result:\", out[\"result\"])\n",
        "            else:\n",
        "                print(\"Error:\", out[\"error\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Cm7W7H1tO6gs"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from functools import lru_cache\n",
        "\n",
        "class LLMMathWrapper:\n",
        "    def __init__(self, model, tokenizer, math_solver):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.math_solver = math_solver\n",
        "\n",
        "    def is_math_prompt(self, prompt):\n",
        "        prompt = prompt.strip()\n",
        "\n",
        "        # Removed punctuation for better word analysis\n",
        "        cleaned = re.sub(r'[^\\w\\s]', '', prompt.lower())\n",
        "\n",
        "        # Quick pass for math symbols or math keywords\n",
        "        if re.search(r'[\\d\\+\\-\\*/\\^=()]', prompt):\n",
        "            math_keywords = {\"solve\", \"evaluate\", \"simplify\", \"integrate\", \"differentiate\", \"factor\"}\n",
        "            if any(kw in cleaned for kw in math_keywords):\n",
        "                return True\n",
        "\n",
        "            words = cleaned.split()\n",
        "            if len(words) <= 5 or sum(w.isdigit() or w in {'x', 'y'} for w in words) >= len(words) // 2:\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    @lru_cache(maxsize=128)\n",
        "    def generate_with_llm(self, prompt):\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        outputs = self.model.generate(inputs, max_length=100, num_return_sequences=1)\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    def run(self, prompt):\n",
        "        if self.is_math_prompt(prompt):\n",
        "            if '=' in prompt:\n",
        "                result = self.math_solver.solve_equation(prompt)\n",
        "                if result[\"success\"]:\n",
        "                    return f\"Symbolic: {result['symbolic']}\\nNumeric: {result['numeric']}\"\n",
        "                else:\n",
        "                    return f\"Math Error: {result['error']}\"\n",
        "            else:\n",
        "                result = self.math_solver.evaluate_expression(prompt)\n",
        "                if result[\"success\"]:\n",
        "                    return f\"Result: {result['result']}\"\n",
        "                else:\n",
        "                    return f\"Math Error: {result['error']}\"\n",
        "        else:\n",
        "            return self.generate_with_llm(prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v6r6l9G_s7iI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "351a36fb-7c8a-4793-df89-67376b483072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain how photosynthesis works. Photosynthesis is the process by which plants, algae, and some bacteria convert light energy into chemical energy in the form of glucose. The process occurs in the chloroplasts of plant cells, where light energy is absorbed by the chlorophyll molecules in the thylakoid membranes. The absorbed light energy is used to split water molecules into hydrogen and oxygen, which are released into the atmosphere as oxygen. The remaining hydrogen is used to produce glucose, which is the primary source of energy for the plant. The process of photosynthesis is a complex and energy-intensive process that requires a lot of energy to produce glucose, which is why plants are often referred to as the \"sun\" of the ecosystem.\n"
          ]
        }
      ],
      "source": [
        "wrapper = LLMMathWrapper(model, tokenizer, solver)\n",
        "print(wrapper.run(\"Explain how photosynthesis works.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2huJutTU5HT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c768c7-396e-4fdb-8eab-d6ffd367dc71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If a bottle costs $3 and you buy 4, how much total? To determine the total cost of buying 4 bottles of soda, we need to know the cost of one bottle. However, since the problem does not specify the cost of one bottle, we will assume that the cost of one bottle is $3. Here is the step-by-step reasoning:\n",
            "\n",
            "1. Identify the cost of one bottle of soda.\n",
            "   - The cost of one bottle is $3.\n",
            "\n",
            "2. Determine the number of bottles being bought.\n",
            "   - You are buying 4 bottles.\n",
            "\n",
            "3. Calculate the total cost by multiplying the cost of one bottle by the number of bottles.\n",
            "   - Total cost = Cost of one bottle Ã— Number of bottles\n",
            "   - Total cost = $3 Ã— 4\n",
            "   - Total cost = $12\n",
            "\n",
            "Therefore, the total cost of buying 4 bottles of soda is \\boxed{12}.\n"
          ]
        }
      ],
      "source": [
        "print(wrapper.run(\"If a bottle costs $3 and you buy 4, how much total?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oVzppPJduLmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade7bf77-0cc1-454d-ecea-d3b6628366d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the capital of France?\n",
            "The capital of France is Paris.\n"
          ]
        }
      ],
      "source": [
        "print(wrapper.run(\"What is the capital of France\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hElaCHEsvL5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37f9eba-84be-4bbe-b332-ce1c19ba4bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbolic: [9/16]\n",
            "Numeric: [0.562500000000000]\n"
          ]
        }
      ],
      "source": [
        "print(wrapper.run(\"sqrt(x) + sqrt(x + 1) = 2\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "id_F1QKlvSUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed6b2ad-ff39-47ee-9a69-516d01114e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbolic: [-sqrt(3), sqrt(3)]\n",
            "Numeric: [-1.73205080756888, 1.73205080756888]\n"
          ]
        }
      ],
      "source": [
        "print(wrapper.run(\"(x - sqrt(3))*(x + sqrt(3)) = 0\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5SML3hxY8r7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "975a9a84-55cc-4755-e0a0-cef01f76eaf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbolic: [-2, 2]\n",
            "Numeric: [-2.00000000000000, 2.00000000000000]\n"
          ]
        }
      ],
      "source": [
        "print(wrapper.run(\"x^2 - 4 = 0\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6z8btzmoEDmc"
      },
      "outputs": [],
      "source": [
        "def reward_func(prompts, completions, completion_ids=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Compute reward for each (prompt, completion) pair.\n",
        "    Returns a list of floats of the same length as completions.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    math_solver = MathSolver()\n",
        "\n",
        "    for prompt, output in zip(prompts, completions):\n",
        "        # Basic check if it's a math prompt\n",
        "        is_math = '=' in prompt or any(op in prompt for op in ['+', '-', '*', '/', '^'])\n",
        "\n",
        "        if is_math:\n",
        "            try:\n",
        "                if '=' in prompt:\n",
        "                    math_result = math_solver.solve_equation(prompt)\n",
        "                    if math_result[\"success\"]:\n",
        "                        # If numeric answer is in output, reward it\n",
        "                        correct_answer = str(math_result[\"numeric\"][0])\n",
        "                        if correct_answer in output:\n",
        "                            rewards.append(1.0)\n",
        "                        else:\n",
        "                            rewards.append(-1.0)\n",
        "                    else:\n",
        "                        rewards.append(-0.5)\n",
        "                else:\n",
        "                    eval_result = math_solver.evaluate_expression(prompt)\n",
        "                    if eval_result[\"success\"] and str(eval_result[\"result\"]) in output:\n",
        "                        rewards.append(1.0)\n",
        "                    else:\n",
        "                        rewards.append(-1.0)\n",
        "            except Exception:\n",
        "                rewards.append(-1.0)\n",
        "        else:\n",
        "            # For non-math prompts, reward any decent length response\n",
        "            rewards.append(0.5 if len(output.strip()) > 10 else -0.5)\n",
        "\n",
        "    return rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SKSf8B_wIXo_"
      },
      "outputs": [],
      "source": [
        "train_dataset = [\n",
        "    {\"prompt\": \"Solve x^2 - 4 = 0\"},\n",
        "    {\"prompt\": \"Tell me a joke\"},\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAXDaKB9KgIc",
        "outputId": "98fbc495-1b47-42bd-c825-389ab9d8c823"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.10.1)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
            "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.56.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.35.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "h6_9dWOLEGVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0262693d-7423-441e-e293-fcc43b097615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1619: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "USE_GPU = False\n",
        "\n",
        "config = GRPOConfig(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_generations=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=5e-6,\n",
        "    logging_steps=2,\n",
        "    no_cuda=not USE_GPU,\n",
        "    report_to = \"none\"\n",
        ")\n",
        "\n",
        "# Instantiate GRPOTrainer\n",
        "grpo_trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    args=config,\n",
        "    reward_funcs=reward_func,\n",
        "    train_dataset=train_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T5t_3B8MxxPj"
      },
      "outputs": [],
      "source": [
        "test_set = [\n",
        "    {\"prompt\": \"x^2 - 4 = 0\"},\n",
        "    {\"prompt\": \"2 + 2 * 3\"},\n",
        "    {\"prompt\": \"Tell me a joke\"},\n",
        "    {\"prompt\": \"Explain how photosynthesis works.\"}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9feCCHTmxsWl"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer, test_set):\n",
        "    model.eval()\n",
        "\n",
        "    wrapper = LLMMathWrapper(model, tokenizer, MathSolver())\n",
        "    prompts = [item[\"prompt\"] for item in test_set]\n",
        "\n",
        "    completions = []\n",
        "    for prompt in prompts:\n",
        "        try:\n",
        "            completions.append(wrapper.run(prompt))\n",
        "        except Exception as e:\n",
        "            completions.append(f\"[Error in generation: {e}]\")\n",
        "\n",
        "    rewards = reward_func(prompts, completions)\n",
        "\n",
        "    return list(zip(prompts, completions, rewards))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QgheErgLxukm"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(results):\n",
        "    correct = sum(1 for _, _, reward in results if reward > 0)\n",
        "    total = len(results)\n",
        "    return correct / total if total > 0 else 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "C5DnXQEljm4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "outputId": "77ceaf91-cf2e-4322-eab8-a71e3f5544aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model BEFORE GRPO training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting GRPO training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/trainer.py:3964: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  ctx_manager = torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 06:38, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed.\n",
            "\n",
            "Evaluating model AFTER GRPO training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ACCURACY REPORT ===\n",
            "Before GRPO Accuracy: 100.00%\n",
            "After  GRPO Accuracy: 100.00%\n",
            "==============================\n",
            "Prompt: x^2 - 4 = 0\n",
            "Output: Symbolic: [-2, 2]\n",
            "Numeric: [-2.00000000000000, 2.00000000000000]\n",
            "Reward: 1.0\n",
            "==============================\n",
            "Prompt: 2 + 2 * 3\n",
            "Output: Result: 8.00000000000000\n",
            "Reward: 1.0\n",
            "==============================\n",
            "Prompt: Tell me a joke\n",
            "Output: Tell me a joke about a cat.\n",
            "\n",
            "Why did the cat go to the vet?\n",
            "\n",
            "Because it was feeling a bit \"catatonic\"!\n",
            "Reward: 0.5\n",
            "==============================\n",
            "Prompt: Explain how photosynthesis works.\n",
            "Output: Explain how photosynthesis works. Photosynthesis is the process by which plants, algae, and some bacteria convert light energy into chemical energy in the form of glucose. The process occurs in the chloroplasts of plant cells, where light energy is absorbed by the chlorophyll molecules in the thylakoid membranes. The absorbed light energy is used to split water molecules into hydrogen and oxygen, which are released into the atmosphere as oxygen. The remaining hydrogen is used to produce glucose, which is the main source of energy for the plant. The process of photosynthesis is a complex and energy-intensive process that requires a lot of energy to produce glucose, which is why plants are often referred to as the \"sun\" of the ecosystem.\n",
            "Reward: 0.5\n"
          ]
        }
      ],
      "source": [
        "# === RUNNING EVALUATION BEFORE TRAINING ===\n",
        "print(\"Evaluating model BEFORE GRPO training...\")\n",
        "pre_results = evaluate_model(model, tokenizer, test_set.copy())\n",
        "pre_accuracy = compute_accuracy(pre_results)\n",
        "\n",
        "# === TRAINING MODEL ON TRAIN DATASET ===\n",
        "print(\"\\nStarting GRPO training...\")\n",
        "grpo_trainer.train()\n",
        "print(\"Training completed.\\n\")\n",
        "\n",
        "# === RUNNING EVALUATION AFTER TRAINING ===\n",
        "print(\"Evaluating model AFTER GRPO training...\")\n",
        "post_results = evaluate_model(model, tokenizer, test_set.copy())\n",
        "post_accuracy = compute_accuracy(post_results)\n",
        "\n",
        "# === PRINT RESULTS ===\n",
        "print(\"\\n=== ACCURACY REPORT ===\")\n",
        "print(f\"Before GRPO Accuracy: {pre_accuracy:.2%}\")\n",
        "print(f\"After  GRPO Accuracy: {post_accuracy:.2%}\")\n",
        "\n",
        "# === Print detailed outputs ===\n",
        "for prompt, output, reward in post_results:\n",
        "    print(\"=\"*30)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Output: {output}\")\n",
        "    print(f\"Reward: {reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7imFVwCEK2jL"
      },
      "execution_count": 17,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3XKh6+UHjXv3HhaXMU2AQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}